{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9292690b",
   "metadata": {},
   "source": [
    "向量是同时拥有大小和方向的量。向量可以表示为排成一排的数字集合，在python中可以处理为一维数组。与此相对，矩阵是排成二维形状的数字集合。\n",
    "\n",
    "向量：一维数组\n",
    "矩阵：二维数组\n",
    "向量的两种表达方式：\n",
    "1. 列向量：每个向量只有一个列，多个行。\n",
    "2. 行向量：每个向量只有一个行，多个列。\n",
    "\n",
    "将向量和数组扩展到N维数据集合，就得到了张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d1281",
   "metadata": {},
   "source": [
    "在python 实现中，在将向量作为行向量处理的情况下，会将向量明确变形为水平方向上的矩阵。如当向量的元素个数是N时，会将向量明确变形为1行N列的二维数组（矩阵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3,)\n",
      "1\n",
      "(2, 3)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([1,2,3])\n",
    "print(x.__class__)\n",
    "print(x.shape)        #表示为（3，），3行，1列。\n",
    "print(x.ndim)   \n",
    "\n",
    "W=np.array([[1,2,3],[4,5,6]])\n",
    "print(W.shape)       #表示为（2，3），2行3列。\n",
    "print(W.ndim)        #表示维数\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  5]\n",
      " [ 7  9 11]]\n",
      "[[ 0  2  6]\n",
      " [12 20 30]]\n"
     ]
    }
   ],
   "source": [
    "#矩阵对应元素的运算\n",
    "W = np.array([[1,2,3],[4,5,6]])\n",
    "X = np.array([[0,1,2],[3,4,5]])\n",
    "print (W+X)\n",
    "print (W*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 20]\n",
      " [30 40]]\n",
      "[[10 40]\n",
      " [30 80]]\n"
     ]
    }
   ],
   "source": [
    "#广播\n",
    "A=np.array([[1,2],[3,4]])\n",
    "print(A*10)                   #标量10先被扩展为2*2的每个元素都是10 的矩阵，然后再进行矩阵对应元素的运算\n",
    "\n",
    "b=np.array([10,20])           #b 被扩展为[10,20] [10,20]\n",
    "print(A*b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f1431",
   "metadata": {},
   "source": [
    "像这样，因为numpy 有广播功能，所以可以智能地执行不同形状数组之间的运算，为了使广播生效，多维数组的形状需要满足几个规则：\n",
    "NumPy广播机制需要满足以下规则才能正确执行不同形状数组之间的运算：\n",
    "\n",
    "1. 形状对齐 ：从数组形状的最右边开始比较，维度大小必须满足以下条件之一：\n",
    "   \n",
    "   - 相等\n",
    "   - 其中一个为1\n",
    "   - 其中一个不存在\n",
    "2. 维度扩展 ：如果数组的维度数不同，形状会在较小的数组左边补1\n",
    "3. 大小匹配 ：在所有维度上，最终广播后的形状大小是各维度上的最大值\n",
    "\n",
    "A = np.array([[1,2,3]])  # 形状(1,3)\n",
    "B = np.array([[1],[2]])   # 形状(2,1)\n",
    "\n",
    "广播后两个数组都变成形状(2,3)\n",
    "A广播为 [[1,2,3], [1,2,3]]\n",
    "B广播为 [[1,1,1], [2,2,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1a23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "#向量内积和矩阵乘积\n",
    "a=np.array([1,2,3])\n",
    "b=np.array([4,5,6])\n",
    "print(np.dot(a,b))\n",
    "\n",
    "A=np.array([[1,2],[3,4]])\n",
    "B=np.array([[5,6],[7,8]])\n",
    "print(np.dot(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6ee41",
   "metadata": {},
   "source": [
    "#### 神经网络全过程\n",
    "\n",
    "以权重层为2的3层神经网络为例(输入层2个神经元，隐藏层4个神经元，输出层3个神经元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "400fa0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.2443906   0.60787232 -0.35772278  0.86966333]\n",
      " [ 2.22411258 -1.0597345   1.73337104 -0.67734531]\n",
      " [ 1.62262345 -2.41246642  3.5923763  -2.45249976]\n",
      " [-0.33644715 -0.27271471  1.4614562  -2.23274136]\n",
      " [ 0.35815312  1.91273437 -1.46481901  0.38870481]\n",
      " [ 0.33119558  1.67811034 -1.16390698  0.14961328]\n",
      " [-1.00682259  1.42346765 -0.47324481 -1.27369954]\n",
      " [ 0.33508     0.17912225  0.70959635 -1.22130757]\n",
      " [-0.41155363  1.48010315 -0.70966527 -0.69242652]\n",
      " [ 0.39596927  0.02598071  0.8841716  -1.30757217]]\n"
     ]
    }
   ],
   "source": [
    "#神经元的计算\n",
    "import numpy as np\n",
    "w1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(4)\n",
    "x1 = np.random.randn(10,2)\n",
    "h1 = np.dot(x1,w1)+b1       #NumPy会自动将 b1 广播为(10,4)的形状，使其可以与矩阵相加\n",
    "print (h1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc81677",
   "metadata": {},
   "source": [
    "以上代码中，10笔样本数据分别由全连接层进行变换，x的第一个维度对应于各笔样本数据。x[0]是第0笔输入，x[1]是第1笔输入数据，h[0]是第0笔数据的隐藏层神经元，h[1]是第1笔数据的隐藏层神经元。\n",
    "以上代码中，偏执b1 的加法运算会触发广播功能，b1 会被扩展为(10,4)的形状，然后与矩阵h相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfdfe0",
   "metadata": {},
   "source": [
    "全连接层的变化换线性变换。激活函数赋予它”非线性“的性质。使用非线性的激活函数，可以增强神经网络的表现力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa548d",
   "metadata": {},
   "source": [
    "以下代码中，x的形状是（10，3），表示10笔数据为一个mini-batch,最终输出的形状是（10，3），每笔数据输出3个类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "x = np.random.randn(10,2)\n",
    "w1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(4)\n",
    "h1 = np.dot(x,w1)+b1\n",
    "a1 = sigmoid(h1)\n",
    "w2 = np.random.randn(4,3)\n",
    "b2 = np.random.randn(3)\n",
    "h2 = np.dot(a1,w2)+b2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba5112",
   "metadata": {},
   "source": [
    "层的类化以及正向传播的实现，将各层实现为pythn类，将主要的变化实现为类的forward()方法\n",
    "\n",
    "X ---- Affine ---- Sigmoid ---- Affine ---- 输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1872104  -1.9367686   0.22096956]\n",
      " [ 0.12997766 -1.91749131 -0.00493773]\n",
      " [ 0.82979008 -1.47656096  0.75801375]\n",
      " [ 0.743995   -1.51217622  0.53739081]\n",
      " [ 0.39657386 -1.86411432  0.49325479]\n",
      " [ 0.30190913 -1.64795461 -0.28443226]\n",
      " [ 0.39064313 -1.75284297  0.24394451]\n",
      " [ 0.22453274 -1.81618982  0.04355382]\n",
      " [ 0.57123394 -1.61003869  0.32408261]\n",
      " [ 0.82166332 -1.46155669  0.66668179]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        # 初始化权重和偏置\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = [Affine(W1, b1),Sigmoid(),Affine(W2, b2)]\n",
    "\n",
    "        # 将所有的权重整理到列表中\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127565e2",
   "metadata": {},
   "source": [
    "这样可以求出输入数据x的得分s ,另外要学习的参数汇总在model.params中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11694c7e",
   "metadata": {},
   "source": [
    "X ---- Affine ---- Sigmoid ---- Affine ---- Softmax ---- CrossEntropyError(损失函数：交叉熵误差) ---- L\n",
    "                                                               |\n",
    "                                                            监督标签t\n",
    "\n",
    "softmax层输出的是概率，在有mini-batch 情况下的交叉熵误差将表示单笔数据的损失函数扩展到了N笔，利用公式除以N ,就可以求单笔数据的平均损失，通过这样的平均化，无论mini-batch大小，都始终可以获得一致的指标。\n",
    "\n",
    "误差反向传播   导数和梯度   链式法则    计算图（加法节点，乘法节点，分支节点:反向传播是上游传来的梯度之和）    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac9181",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
